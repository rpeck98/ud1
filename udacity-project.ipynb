{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Udacity Azure Machine Learning Engineer - Project 1\n",
        "\n",
        "**Name:** Bob Peck\n",
        "\n",
        "**Date:** March 5, 2023\n",
        "\n",
        "This is the Jupyter Notebook associated with the Udacity Azure Machine Learning Project 1. The objective of this project is to compare a custom-coded model (using Scikit-learn Logistic Regression) and an AutoML model. For the custom-coded model, I'll use HyperDrive to optimize the hyperparameters, targeting *accuracy* as the primary metric. For the AutoML model, I'll supply the same dataset and let AutoML select the best model and hyperparameters. I'll limit the time for optimization simply to manage costs on the compute.\n",
        "\n",
        "## Setup the workspace, compute and experiment\n",
        "\n",
        "These next following sections I'll provision the components necessary to conduct the ML experiments.\n",
        "\n",
        "- Get a reference to the previously provisioned ML Workspace\n",
        "- Setup the compute cluster for the ML experiments\n",
        "\n",
        "Once those are complete, then I'll begin to define the first experiment - the custom-coded ML HyperDrive experiment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678149093037
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# \n",
        "# This provisioning uses the STANDARD_D2_V2 vm size for cost management purposes.\n",
        "# We could have selected a larger vm for the cluster for more compute to conduct more concurrent experiments\n",
        "# \n",
        "\n",
        "cluster_name = \"bank-marketing-cluster\"\n",
        "compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", min_nodes=0, max_nodes=4)\n",
        "\n",
        "try:\n",
        "    my_compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    my_compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "    my_compute_target.wait_for_completion(show_output=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678149093212
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the HyperDrive experiment\n",
        "\n",
        "This section sets up the HyperDrive experiment. Key hyperparameters to experiment with are the values for *C* and *max_iter*\n",
        "\n",
        "Analysis of values for C and max_iter (following is from [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)):\n",
        "\n",
        "- C is the inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. \n",
        "- max_iter is maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "Observations:\n",
        "\n",
        "- Tested a variety of values for C, ranging from 0.01 to 100. Lower values produced higher accuracy scores, hence narrowed the range on later runs to 0.001 to 1.\n",
        "- Tested a variety of values for max_iter, ranging from 100 to 1600. Seemingly past ~1000, the algorithm failed to gain any more accuracy during further iterations. Optimal value seems to be around 600-800, depending on the C value.\n",
        "- Tested higher values for max_total_runs, but didn't observe any higher accuracy with more runs.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.sklearn import SKLearn\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
        "from azureml.core import Environment, ScriptRunConfig\n",
        "import os\n",
        "\n",
        "# Specify parameter sampler\n",
        "ps = RandomParameterSampling({\n",
        "    'C': uniform(0.001, 1),\n",
        "    'max_iter': choice(100, 200, 400, 800)\n",
        "})\n",
        "\n",
        "# Specify a Policy\n",
        "policy = BanditPolicy(slack_factor=0.2, evaluation_interval=1)\n",
        "\n",
        "if \"training\" not in os.listdir():\n",
        "    os.mkdir(\"./training\")\n",
        "\n",
        "# Setup environment for your training run\n",
        "sklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='conda_dependencies.yml')\n",
        "\n",
        "# Create a ScriptRunConfig Object to specify the configuration details of your training job\n",
        "src = ScriptRunConfig(source_directory='.', script='train.py', environment=sklearn_env, compute_target=my_compute_target)\n",
        "\n",
        "# Create a HyperDriveConfig using the src object, hyperparameter sampler, and policy.\n",
        "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
        "                                     hyperparameter_sampling=ps,\n",
        "                                     primary_metric_name='Accuracy',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                     max_total_runs=20,\n",
        "                                     max_concurrent_runs=10,\n",
        "                                     policy=policy)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678153169051
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the HyperDrive experiment \n",
        "\n",
        "With all the hyperparameters set, we now submit the experiment for execution.\n",
        "\n",
        "I've chosen to use *wait_for_completion()* method to prevent the next section from executing prior to this being done. This was a personal choice and not specifically needed for successful experiments.\n",
        "\n",
        "### Results\n",
        "\n",
        "The LogisticRegression ML model with given hyperparameters seems to find a maximum accuracy score around 90.9%. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit your hyperdrive run to the experiment and show run details with the widget.\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "exp = Experiment(workspace=ws, name=\"udacity-project\")\n",
        "\n",
        "# Submit the HyperDriveConfig object to run the experiment\n",
        "hyperdrive_run = exp.submit(config=hyperdrive_config, show_output=False)\n",
        "\n",
        "# Use the RunDetails widget to display the run details\n",
        "RunDetails(hyperdrive_run).show()\n",
        "hyperdrive_run.wait_for_completion(show_output=False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678153773400
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the best model from HyperDrive experiment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Get your best run and save the model from that run.\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "print(best_run.get_details()['runDefinition']['arguments'])\n",
        "print(best_run.get_file_names())\n",
        "\n",
        "best_run.register_model(model_name='hyperdrive-bank', model_path='outputs/model.joblib')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678153830204
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the AutoML experiment\n",
        "\n",
        "These next sections setup the AutoML experiment for execution using the same data.\n",
        "\n",
        "For AutoML, no model is explicity chosen by the ML engineer - the AutoML capabilities select the best model and hyperparameter combinations. This greatly speeds the delivery of an optimal ML model for the given dataset and objectives."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the data\n",
        "\n",
        "Here we prepare the data by \n",
        "\n",
        "1. retrieving it from the URI and creating a TabularDataset object.\n",
        "2. Cleaning the data as in the previous experiment\n",
        "3. Joining the x and y dataframes back together and converting them into a TabularDataset for AutoML purposes\n",
        "\n",
        "While this is likely not an optimal process, I'll use it here for expedience with the given code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "\n",
        "# Create TabularDataset using TabularDatasetFactory\n",
        "# Data is available at: \n",
        "# \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\n",
        "\n",
        "ds = TabularDatasetFactory.from_delimited_files(path=\"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\", separator=\",\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678149892525
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from train import clean_data\n",
        "\n",
        "# Use the clean_data function to clean your data.\n",
        "x, y = clean_data(ds)\n",
        "\n",
        "x_complete = x.join(y)\n",
        "\n",
        "default_ds = ws.get_default_datastore()\n",
        "x_tab_ds = TabularDatasetFactory.register_pandas_dataframe(dataframe=x_complete, target=default_ds, name=\"Bank Marketing Data\", show_progress=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678149897136
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup parameters for the AutoML experiment\n",
        "\n",
        "I found this section to have the most options to consider - thankfully Microsoft provides great documentation on [*How to Configure AutoML Training*](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#primary-metric)\n",
        "\n",
        "Selections made here include:\n",
        "\n",
        "- task --> classification\n",
        "- primary_metric --> accuracy\n",
        "- cross_validations --> 3\n",
        "\n",
        "AutoML does the rest!\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "# Set parameters for AutoMLConfig\n",
        "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
        "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
        "# Azure tenant, which will incur personal costs.\n",
        "automl_config = AutoMLConfig(\n",
        "    experiment_timeout_minutes=30,\n",
        "    task=\"classification\",\n",
        "    compute_target=my_compute_target,\n",
        "    primary_metric=\"accuracy\",\n",
        "    training_data=x_tab_ds,\n",
        "    label_column_name=\"y\",\n",
        "    n_cross_validations=3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678150148382
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the AutoML job\n",
        "\n",
        "AutoML goes and does its work now. \n",
        "\n",
        "**Observations:** Primary observation is that AutoML selected the same \"best\" algorithm each time. The first run I tried, selected a *VotingEnsaemble* ML algorithm as the best (highest accuracy). The second run also selected *VotingEnsemble* as the best algorithm. Further experiments may select a different algorithm with additional time allocated (future work).\n",
        "\n",
        "**Results:** the AutoML experiment was able to achieve a slightly higher accuracy score, ~91.7 utilizing a VotingEnsemble"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit your automl run\n",
        "\n",
        "remote_run = exp.submit(automl_config, show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678152761924
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the best model\n",
        "\n",
        "Final step is to save the best model (as measured by accuracy as the primary metric)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Get your best run and save the model from that run.\n",
        "\n",
        "best_run = remote_run.get_best_child(metric='accuracy')\n",
        "print(best_run.get_details()['runDefinition']['arguments'])\n",
        "print(best_run.get_file_names())\n",
        "\n",
        "best_run.register_model(model_name='automl-bank', model_path='outputs/model.pkl')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678152925086
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete the compute resource "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    my_compute_target.delete()\n",
        "    my_compute_target.wait_for_completion(show_output=True)\n",
        "except ComputeTargetException:\n",
        "    print('ComputeTarget not found')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1678154003491
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e7ba531129155df8dec46b9996bea1b7914958870c64b8e8feeb155f5ef9260"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}